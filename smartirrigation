import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data_path = "/content/drive/MyDrive/Irrigation/dummy_smart_irrigation_dataset_with_water.csv"
data = pd.read_csv(data_path)

# Debugging target variable
def inspect_target(target_column):
    print("Target variable preview:")
    print(data[target_column].head())
    print(f"Data type: {data[target_column].dtype}")
    print(f"Null values: {data[target_column].isnull().sum()}")
    print(f"Unique values: {data[target_column].unique()}")

# Encode categorical variables
label_encoder = LabelEncoder()
data["Soil Type"] = label_encoder.fit_transform(data["Soil Type"])
data["Growth Stage of Rice"] = label_encoder.fit_transform(data["Growth Stage of Rice"])
data["Crop"] = label_encoder.fit_transform(data["Crop"])

# Inspect target variable
inspect_target("Water Requirement (liters)")

# Define features and target
features = data.drop(columns=["Water Requirement (liters)"])  # All features except target
target = data["Water Requirement (liters)"]

# Handle potential non-numeric target issues
if target.dtype not in [np.float64, np.int64]:
    try:
        target = target.astype(float)
    except ValueError:
        print("Error: Target variable could not be converted to numeric.")
        exit()

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42, n_estimators=100),
    "SVM": SVR(kernel='rbf'),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42, n_estimators=100)
}

# Train and evaluate models
results = []

for name, model in models.items():
    try:
        # Train the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Evaluate the model
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)

        # Store results
        results.append({"Model": name, "MAE": mae, "MSE": mse, "RMSE": rmse, "R2": r2})
    except Exception as e:
        print(f"Error training {name}: {e}")

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Plot results
plt.figure(figsize=(12, 6))
sns.barplot(x="Model", y="R2", data=results_df, palette="viridis")
plt.title("Model Comparison by R2 Score")
plt.ylabel("R2 Score")
plt.xlabel("Model")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
sns.barplot(x="Model", y="RMSE", data=results_df, palette="viridis")
plt.title("Model Comparison by RMSE")
plt.ylabel("Root Mean Squared Error")
plt.xlabel("Model")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Feature importance for tree-based models
if "Random Forest" in models:
    rf_model = models["Random Forest"]
    importances = rf_model.feature_importances_
    feature_names = features.columns
    importance_df = pd.DataFrame({"Feature": feature_names, "Importance": importances}).sort_values(by="Importance", ascending=False)

    plt.figure(figsize=(12, 6))
    sns.barplot(x="Importance", y="Feature", data=importance_df, palette="viridis")
    plt.title("Feature Importance (Random Forest)")
    plt.xlabel("Importance")
    plt.ylabel("Feature")
    plt.tight_layout()
    plt.show()

# Display results
print(results_df)

# Save results to CSV
results_df.to_csv("model_comparison_results.csv", index=False)
